{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "def tokenize_paragraph(paragraph):\n",
        "    sentences = nltk.sent_tokenize(paragraph)\n",
        "    words = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "    return sentences, words\n",
        "\n",
        "text_data = \"This is a sample paragraph. It contains multiple sentences for tokenization. Let's tokenize this text into words and sentences!\"\n",
        "\n",
        "sentences, words = tokenize_paragraph(text_data)\n",
        "print(\"Sentences:\", sentences)\n",
        "print(\"Words:\", words)\n"
      ],
      "metadata": {
        "id": "jnpH8jv8yDP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "POS Tagging: Tags each word with its part of speech.\n",
        "Lemmatization: Converts words to their base forms based on POS tags.\n",
        "Detailed Outputs: Displays original text, tokenized sentences, words, POS tags, and lemmatized sentences."
      ],
      "metadata": {
        "id": "ESvs2vdqyn_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It uses NLTK's sent_tokenize and word_tokenize functions for this purpose."
      ],
      "metadata": {
        "id": "sn3-ao8Ly6oy"
      }
    }
  ]
}